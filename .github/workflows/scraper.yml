name: Scraper Pipeline

on:
  schedule:
    # Run daily at 2:00 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      mode:
        description: "Discovery mode: creator-discovery (recommended) or brand-discovery (legacy)"
        required: false
        type: choice
        options:
          - creator-discovery
          - brand-discovery
          - both
        default: creator-discovery
      strategies:
        description: "Creator discovery strategies (comma-separated): search, channel, sponsorblock"
        required: false
        type: string
        default: "search,channel"
      max_results:
        description: "Max results per search query"
        required: false
        type: string
        default: "10"
      max_videos:
        description: "Max videos per creator channel"
        required: false
        type: string
        default: "10"
      skip_youtube:
        description: "[Legacy] Skip YouTube discovery (brand-discovery mode only)"
        required: false
        type: boolean
        default: false
      categories:
        description: "[Legacy] Brand categories (brand-discovery mode only)"
        required: false
        type: string
        default: ""
      discovery_only:
        description: "Only discover, skip full scraping"
        required: false
        type: boolean
        default: false

jobs:
  scrape:
    name: Discover creators & scrape codes
    runs-on: ubuntu-latest
    timeout-minutes: 60
    defaults:
      run:
        working-directory: scraper
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip
          cache-dependency-path: scraper/requirements.txt

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Create .env file
        run: |
          echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" > .env
          echo "SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" >> .env

      # Creator-centric discovery (primary â€” recommended)
      - name: Run creator discovery
        if: ${{ inputs.mode == 'creator-discovery' || inputs.mode == 'both' || inputs.mode == '' }}
        run: |
          python scraper.py discover-creators \
            --strategies "${{ inputs.strategies || 'search,channel' }}" \
            --max-results "${{ inputs.max_results || '10' }}" \
            --max-videos "${{ inputs.max_videos || '10' }}" \
            2>&1 | tee creator-discovery.log

      # Legacy brand discovery (kept for backward compatibility)
      - name: Run brand discovery (legacy)
        if: ${{ inputs.mode == 'brand-discovery' || inputs.mode == 'both' }}
        run: |
          ARGS=""
          if [ "${{ inputs.skip_youtube }}" = "true" ]; then
            ARGS="$ARGS --skip-youtube"
          fi
          if [ -n "${{ inputs.categories }}" ]; then
            ARGS="$ARGS --categories ${{ inputs.categories }}"
          fi
          if [ "${{ inputs.discovery_only }}" = "true" ]; then
            ARGS="$ARGS --discovery-only"
          fi
          python brand_discovery.py $ARGS 2>&1 | tee brand-discovery.log

      - name: Run full scraper for all brands
        if: ${{ inputs.discovery_only != true && github.event.inputs.discovery_only != 'true' && (inputs.mode == 'brand-discovery' || inputs.mode == 'both') }}
        run: |
          python scraper.py 2>&1 | tee scraper.log

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: |
            scraper/creator-discovery.log
            scraper/brand-discovery.log
            scraper/discovery.log
            scraper/scraper.log
          retention-days: 30
          if-no-files-found: ignore
