name: Scraper Pipeline

on:
  schedule:
    # Run daily at 2:00 AM UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      skip_youtube:
        description: "Skip YouTube discovery (faster, coupon sites + curated list only)"
        required: false
        type: boolean
        default: false
      categories:
        description: "Comma-separated categories (e.g., fitness,beauty,tech). Leave empty for all."
        required: false
        type: string
        default: ""
      discovery_only:
        description: "Only discover brands, skip code scraping"
        required: false
        type: boolean
        default: false

jobs:
  scrape:
    name: Discover brands & scrape codes
    runs-on: ubuntu-latest
    timeout-minutes: 60
    defaults:
      run:
        working-directory: scraper
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip
          cache-dependency-path: scraper/requirements.txt

      - name: Install Python dependencies
        run: pip install -r requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Create .env file
        run: |
          echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" > .env
          echo "SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}" >> .env

      - name: Run brand discovery
        run: |
          ARGS=""
          if [ "${{ inputs.skip_youtube }}" = "true" ]; then
            ARGS="$ARGS --skip-youtube"
          fi
          if [ -n "${{ inputs.categories }}" ]; then
            ARGS="$ARGS --categories ${{ inputs.categories }}"
          fi
          if [ "${{ inputs.discovery_only }}" = "true" ]; then
            ARGS="$ARGS --discovery-only"
          fi
          python brand_discovery.py $ARGS 2>&1 | tee discovery.log

      - name: Run full scraper for all brands
        if: ${{ inputs.discovery_only != true && github.event.inputs.discovery_only != 'true' }}
        run: |
          python scraper.py 2>&1 | tee scraper.log

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: |
            scraper/discovery.log
            scraper/scraper.log
          retention-days: 30
          if-no-files-found: ignore
